import streamlit as st

st.set_page_config(
    page_title="ReadMe", page_icon="📖", initial_sidebar_state="collapsed"
)

st.session_state["readme_viewed"] = True

st.header("📖 ReadMe")
sections = (
    "💊 About TabLit",
    "💾 Assets",
    "🛠️ Langsmith integration",
    "⚠️ Precautions",
    "🤔 LLM Pain Points",
)
tabs = st.tabs(sections)

readme1 = """
- TabLit provides an interface for chatting with Tabular Data using a LLM. 
- Tabular data can be through:
    - Uploading CSV or excel files.
    - Crawling ecommerce (Amazon and Walmart) and SERP (Google) sites for a keyword.
- The uploaded or crawled data can be explored using the `Explore data?` option.
- This data will be used to answer questions asked to the chatbot.
- The steps taken to solve the problem along with its thought process (Internal Monologue) is displayed.
- Conversation history can be cleared when required.
"""
tabs[0].subheader(sections[0])
tabs[0].info(readme1)

readme2 = """
- Assets:
    - Crawled data.
    - Files generated by the bot, ex: plots and graphs.
    - Chat export including inputs, intermediate steps taken by the agent and output.
- The required assets can be selected from the dropdown.
- This will be zipped and downloaded.
"""
tabs[1].subheader(sections[1])
tabs[1].success(readme2)

readme3 = """
- The app is also integrated with Langsmith for debugging, testing, evaluating, and monitoring.
- Each trace and prompt can be viewed.
- User feedback and score is collected as well, for improving the app.
- The latest trace link is displayed on the sidebar for easier access.
"""
tabs[2].subheader(sections[2])
tabs[2].info(readme3)

readme4 = """
- This app is not "production ready", as it may have few Vulnerabilities.
- LangChain's PythonAstREPLTool is vulnerable to arbitrary code execution.
- It may be subject to LLM attacks like Prompt injection.
- Custom prompt is being used to reduce the same but is not foolproof.
- Periodically rotating API keys and tokens is suggested, to minimise damage, in case it gets leaked.
- Setting Budget limits along with limited permission to tokens is better.
"""
tabs[3].subheader(sections[3])
tabs[3].error(readme4)

readme5 = """
LLM Pain Points and how this app tries to address them:
 - Transparency : Open source app with LangSmith integration, exact prompts being sent to LLM can be monitored and debugged.
 - Trust : User feedback and rating is being captured to fine tune the LLM responses and make the app better.
 - Accuracy  : Custom prompt attempts to increase accuracy by addressing common failure points.
 - Privacy : Streamlit secrets is being used to store all tokens and API keys.
 - Cost Reduction : Many functions are cached to prevent them from executing when not required.
"""
tabs[4].subheader(sections[4])
tabs[4].success(readme5)
